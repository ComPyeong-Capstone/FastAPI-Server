import requests
import json
import base64
import os
import whisper
from pydub import AudioSegment
from dotenv import load_dotenv
import time
# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ğŸ”¹ Google Cloud API í‚¤
GOOGLE_TTS_API_KEY = os.getenv("GOOGLE_TTS_API_KEY")

# ğŸ”¹ API ìš”ì²­ URL
url = f"https://texttospeech.googleapis.com/v1/text:synthesize?key={GOOGLE_TTS_API_KEY}"

# ğŸ”¹ TTS ìŒì„±ì´ ì €ì¥ë  í´ë” ê²½ë¡œ
output_folder = os.path.expanduser("music")

# í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs(output_folder, exist_ok=True)

def get_next_filename():
    # í˜„ì¬ í´ë”ì— ì €ì¥ëœ TTS íŒŒì¼ ëª©ë¡ì„ í™•ì¸í•˜ê³ ,
    # 'tts_output_X.mp3' í˜•ì‹ì˜ íŒŒì¼ëª…ì„ ìë™ìœ¼ë¡œ ì¦ê°€ì‹œì¼œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

    existing_files = [f for f in os.listdir(output_folder) if f.startswith("tts_output") and f.endswith(".mp3")]
    numbers = [
        int(f.replace("tts_output_", "").replace(".mp3", ""))
        for f in existing_files if f.replace("tts_output_", "").replace(".mp3", "").isdigit()
    ]
    next_number = max(numbers) + 1 if numbers else 1
    return f"tts_output_{next_number}.mp3"

def generate_tts(text):
    
    # Google Cloud Text-to-Speech APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ìŒì„± ë°ì´í„°(MP3)ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜.
    # ë³€í™˜ëœ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë°˜í™˜í•œë‹¤.
    
    voice_name = "ko-KR-Wavenet-C"
    data = {
        "input": {"text": text},
        "voice": {
            "languageCode": "ko-KR",
            "name": voice_name,
            "ssmlGender": "MALE"
        },
        "audioConfig": {
            "audioEncoding": "MP3",
            "speakingRate": 1.25
        }
    }
    response = requests.post(url, headers={"Content-Type": "application/json"}, data=json.dumps(data))
    
    if response.status_code == 200:
        response_data = response.json()
        audio_content = response_data["audioContent"]
        return base64.b64decode(audio_content)
    else:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {response.text}")
        return None



# ë¬¸ì¥ì„ ë‹¨ì–´ ê¸°ì¤€ìœ¼ë¡œ ì•/ë’¤ë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ (í™€ìˆ˜ëŠ” ì•ë¶€ë¶„ì´ ë” ë§ê²Œ)
def split_sentence(sentence):
    words = sentence.strip().split()
    mid = (len(words) + 1) // 2
    return ' '.join(words[:mid]), ' '.join(words[mid:])

# ë©”ì¸ í•¨ìˆ˜: ì…ë ¥ëœ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ TTSë¡œ ë³€í™˜ í›„ í•©ì¹˜ê³  duration ë°°ì—´ ë°˜í™˜
def text_to_speech(text_list):
    if not isinstance(text_list, list):
        raise ValueError("ì…ë ¥ì€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    
    combined_audio = AudioSegment.silent(duration=0)
    start_time = 0
    interval = 5000
    front_durations = []  # ê° ì•ë¶€ë¶„ì˜ duration ì €ì¥

    for idx, text in enumerate(text_list):
        # ë¬¸ì¥ ë¶„ë¦¬
        front_part, back_part = split_sentence(text)

        # ì•ë¶€ë¶„ TTS ìƒì„± ë° ë¶„ì„
        front_tts_data = generate_tts(front_part)
        front_temp_path = os.path.join(output_folder, f"temp_front_{idx}.mp3")
        with open(front_temp_path, "wb") as f:
            f.write(front_tts_data)

        front_duration = analyze_audio_with_whisper(front_temp_path)
        front_durations.append(front_duration)

        # ë¶„ë¦¬ëœ ë‘ ë¶€ë¶„ì„ ë‹¤ì‹œ í•©ì³ì„œ ìµœì¢… TTS ìƒì„±
        merged_text = front_part + " " + back_part
        merged_tts_data = generate_tts(merged_text)

        merged_temp_path = os.path.join(output_folder, f"temp_merged_{idx}.mp3")
        with open(merged_temp_path, "wb") as f:
            f.write(merged_tts_data)

        tts_audio = AudioSegment.from_mp3(merged_temp_path)

        # ì‹œì‘ ì‹œê°„ ë§ì¶”ê¸° ìœ„í•œ ë¬´ìŒ ì¶”ê°€
        silent_gap = AudioSegment.silent(duration=max(0, start_time - len(combined_audio)))
        if start_time >= 5000:
            silent_gap += AudioSegment.silent(duration=500)

        # ìµœì¢… ì˜¤ë””ì˜¤ì— ì¶”ê°€
        combined_audio += silent_gap + tts_audio
        start_time += interval

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        os.remove(front_temp_path)
        os.remove(merged_temp_path)

    # ì „ì²´ ê¸¸ì´ë¥¼ 5ì´ˆ ë‹¨ìœ„ë¡œ ë§ì¶”ê¸°
    final_length_ms = ((len(combined_audio) + 4999) // 5000) * 5000
    if len(combined_audio) < final_length_ms:
        padding_duration = final_length_ms - len(combined_audio)
        combined_audio += AudioSegment.silent(duration=padding_duration)

    # ìµœì¢… íŒŒì¼ ì €ì¥
    output_file = os.path.join(output_folder, get_next_filename())
    combined_audio.export(output_file, format="mp3")
    print(f"âœ… TTS ìŒì„± íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")

    # íŒŒì¼ ê²½ë¡œì™€ ì•ë¶€ë¶„ duration ë°°ì—´ ë°˜í™˜
    return output_file, front_durations


def text_to_speech_with_poping(text_list):
    """
    í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ TTS ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± + ë‹¨ì–´ë³„ íƒ€ì´ë° ë¶„ì„ (poping ìŠ¤íƒ€ì¼)
    ë°˜í™˜: (TTS íŒŒì¼ ê²½ë¡œ, ëª¨ë“  ë‹¨ì–´ íƒ€ì´ë° ë¦¬ìŠ¤íŠ¸)
    """
    if not isinstance(text_list, list):
        raise ValueError("ì…ë ¥ì€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    combined_audio = AudioSegment.silent(duration=0)
    start_time = 0  # milliseconds
    interval = 5000  # 5ì´ˆ ê°„ê²©
    all_word_timings = []

    for idx, text in enumerate(text_list):
        # TTS ìƒì„±
        tts_data = generate_tts(text)
        merged_temp_path = os.path.join(output_folder, f"temp_merged_{idx}.mp3")
        with open(merged_temp_path, "wb") as f:
            f.write(tts_data)

        tts_audio = AudioSegment.from_mp3(merged_temp_path)

        # Whisper ë¶„ì„
        word_timings = analyze_audio_words_with_whisper(merged_temp_path)

        # ğŸ”§ start_time ë³´ì •ì€ ì˜¤ë””ì˜¤ ê²°í•© ì „ì— ì ìš©í•´ì•¼ ì •í™•
        adjusted_word_timings = []
        for w in word_timings:
            adjusted_word_timings.append({
                "word": w["word"],
                "start": round(w["start"] + start_time / 1000, 2),
                "end": round(w["end"] + start_time / 1000, 2)
            })

        all_word_timings.append(adjusted_word_timings)

        # ë””ë²„ê¹… ì¶œë ¥
        print(f"\nğŸ§¾ [ì¸ë±ìŠ¤ {idx}] ë³´ì •ëœ ë‹¨ì–´ íƒ€ì´ë°:")
        for word_info in adjusted_word_timings:
            print(f"ğŸ—£ {word_info['word']} | â± {word_info['start']}s ~ {word_info['end']}s | ê¸¸ì´: {round(word_info['end'] - word_info['start'], 2)}s")

        # ë¬´ìŒ gap ì‚½ì…
        silent_gap = AudioSegment.silent(duration=max(0, start_time - len(combined_audio)))
        if start_time >= 5000:
            silent_gap += AudioSegment.silent(duration=500)

        combined_audio += silent_gap + tts_audio

        # start_time ê°±ì‹ ì€ ë§ˆì§€ë§‰ì—!
        start_time += interval

        os.remove(merged_temp_path)

    # ìµœì¢… ê¸¸ì´ ì •ë¦¬
    final_length_ms = ((len(combined_audio) + 4999) // 5000) * 5000
    if len(combined_audio) < final_length_ms:
        combined_audio += AudioSegment.silent(duration=final_length_ms - len(combined_audio))

    output_file = os.path.join(output_folder, get_next_filename())
    combined_audio.export(output_file, format="mp3")
    print(f"âœ… Poping ìŠ¤íƒ€ì¼ TTS ìŒì„± íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")

    return output_file, all_word_timings



# Whisper ëª¨ë¸ì„ í†µí•´ ì˜¤ë””ì˜¤ì˜ ì•ë¶€ë¶„ durationê³¼ ê° íƒ€ì´ë°ì„ ë¶„ì„í•˜ê³  ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
def analyze_audio_with_whisper(audio_file):
    model = whisper.load_model("base") #whisper model : tiny, base, small, medium, large
    result = model.transcribe(audio_file, word_timestamps=True)

    print(f"\nğŸ” [Whisper ë¶„ì„ ê²°ê³¼: {audio_file}] ğŸ”")
    for idx, segment in enumerate(result["segments"]):
        start = round(segment["start"], 2)
        end = round(segment["end"], 2)
        text = segment["text"]
        print(f"â± ì„¸ê·¸ë¨¼íŠ¸ {idx}: {start}s ~ {end}s | í…ìŠ¤íŠ¸: {text}")

    first_segment = result["segments"][0]
    duration = round(first_segment["end"] - first_segment["start"], 2)
    return duration



def analyze_audio_words_with_whisper(audio_file):
    """
    ğŸ” ì£¼ì–´ì§„ ì˜¤ë””ì˜¤ íŒŒì¼ì„ Whisperë¡œ ë¶„ì„í•´ì„œ
    ë‹¨ì–´ ë‹¨ìœ„ (word-level)ë¡œ (ë‹¨ì–´, ì‹œì‘ì‹œê°„, ëì‹œê°„) ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.

    Args:
        audio_file (str): ë¶„ì„í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (.mp3)

    Returns:
        List[dict]: ê° ë‹¨ì–´ë³„ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            ì˜ˆ: [{"word": "Hello", "start": 0.5, "end": 0.8}, ...]
    """

    # Whisper ëª¨ë¸ ë¡œë“œ (medium ëª¨ë¸ ì‚¬ìš©)
    model = whisper.load_model("base")  #whisper model : tiny, base, small, medium, large

    # ì˜¤ë””ì˜¤ë¥¼ word timestamps ì˜µì…˜ì„ ì¼œê³  ë³€í™˜
    result = model.transcribe(audio_file, word_timestamps=True)

    word_timings = []

    # Whisper ê²°ê³¼ ì¤‘ segmentsë¥¼ ìˆœíšŒ
    for segment in result["segments"]:
        if "words" in segment:
            for word_info in segment["words"]:
                word = word_info.get("word", "").strip()
                start = round(word_info.get("start", 0), 2)
                end = round(word_info.get("end", 0), 2)
                word_timings.append({
                    "word": word,
                    "start": start,
                    "end": end
                })

    return word_timings




# ğŸ”¹ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# subtitles = [
#     "ì½”ë“œë¥¼ ì‘ì„±í•  ë•Œ ì£¼ì„ì„ ì¶©ë¶„íˆ ë‹¬ì§€ ì•ŠëŠ” ì‹¤ìˆ˜ë¥¼ ì¢…ì¢… í•©ë‹ˆë‹¤.",
#     "ë³€ìˆ˜ëª…ì„ ëª…í™•í•˜ì§€ ì•Šê²Œ ì§€ì–´ì„œ ë‚˜ì¤‘ì— í˜¼ë€ì„ ê²ªê²Œ ë˜ì£ .",
#     "ë°±ì—… ì—†ì´ ì¤‘ìš”í•œ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.",
#     "ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì œëŒ€ë¡œ ì½ì§€ ì•Šê³  ë„˜ì–´ê°€ëŠ” ê²½ìš°ë„ í”í•´ìš”."
# ]

# subtitles2 = [
#     "ë™í•´ë¬¼ê³¼ ë°±ë‘ì‚°ì´ ë§ˆë¥´ê³  ë‹³ë„ë¡.",
#     "í•˜ëŠë‹˜ì´ ë³´ìš°í•˜ì‚¬ ìš°ë¦¬ë‚˜ë¼ ë§Œì„¸.",
#     "ë¬´ê¶í™” ì‚¼ì²œë¦¬ í™”ë ¤ê°•ì‚°",
#     "ëŒ€í•œì‚¬ëŒ ëŒ€í•œìœ¼ë¡œ ê¸¸ì´ ë³´ì „í•˜ì„¸"
# ]

# text_to_speech(subtitles)
# text_to_speech(subtitles2)


# {
#   "videos": ["video1.mp4", "video2.mp4"],
#   "subtitles": [
#     "ì½”ë“œë¥¼ ì‘ì„±í•  ë•Œ ì£¼ì„ì„ ì¶©ë¶„íˆ ë‹¬ì§€ ì•ŠëŠ” ì‹¤ìˆ˜ë¥¼ ì¢…ì¢… í•©ë‹ˆë‹¤.",
#     "ë³€ìˆ˜ëª…ì„ ëª…í™•í•˜ì§€ ì•Šê²Œ ì§€ì–´ì„œ ë‚˜ì¤‘ì— í˜¼ë€ì„ ê²ªê²Œ ë˜ì£ ."],
#   "music_url": "bgm_01.mp3"
# }



# 1. í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥
# text = "ì½”ë“œë¥¼ ì‘ì„±í•  ë•Œ ì£¼ì„ì„ ì¶©ë¶„íˆ ë‹¬ì§€ ì•ŠëŠ” ì‹¤ìˆ˜ë¥¼ ì¢…ì¢… í•©ë‹ˆë‹¤."

# # 2. TTS ìƒì„±
# tts_data = generate_tts(text)

# # mp3 íŒŒì¼ë¡œ ì €ì¥
# test_audio_path = os.path.join(output_folder, "test_tts.mp3")
# with open(test_audio_path, "wb") as f:
#     f.write(tts_data)

# # 3. ë‹¨ì–´ë³„ ì½ëŠ” ì‹œê°„ ë¶„ì„
# word_timings = analyze_audio_words_with_whisper(test_audio_path)

# # 4. ê²°ê³¼ ì¶œë ¥
# for info in word_timings:
#     print(f"ğŸ—£ ë‹¨ì–´: {info['word']} | ì‹œì‘: {info['start']}s | ë: {info['end']}s | ê¸¸ì´: {round(info['end'] - info['start'], 2)}s")
